Support vector machine (SVM) is a popular supervised learning method for performing classification on a data set. In a given data set, it is entirely possible that there are many possible hyperplanes that adequately separate the data. However, which one of these separators is optimal? It is not immediately obvious which separator might be optimal without some sort of visualization.

In reality, it is often the case that a linear hyperplane does not exist for the given data. Real data will often not be linearly separable, and therefore the desired hyperplane cannot be found in the original input space. As scientists, we often fear the curse of dimensionality, or the impacts that a large number of features (high dimensionality) will have on the classification algorithm. Increasing the dimensionality in most cases is seen as a poor decision. However, if the transformation to a higher dimension feature space is “smart”, then it can actually be beneficial. This mapping of the data to a high dimensional feature space for SVM is called the kernel trick. The kernel is a function that maps the input data to a higher dimension, which will hopefully make finding the optimal margins easier since the data might be linearly separable in the new feature space. The benefit of the kernel trick is that it is very easy to calculate, which is why SVM is a popular choice for supervised learning. There are many different kernels that are easy to implement, try and optimize.

In this paper, we discuss our experiments to test multiple different kernels on a binary classification data set.

**Paper, collaborator credit, and associated software to be uploaded by end of 2021!**
